{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes from Nielsen course\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "Backprob calculates the error at every neuron, therefore error for weights and biases. \n",
    "\n",
    "Four equations:\n",
    "1. Calculate error for output neurons from output\n",
    "2. Backpropagate error to lower layers\n",
    "3. Calculate error for weights\n",
    "4. Calculate error for biases\n",
    "\n",
    "\n",
    "Advantage is that can calculate errors in a single forward and backward pass. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Cross entropy\n",
    "\n",
    "So far, have been using least squares as cost function, however, this means that learning rate is small when neuron saturates. Not so for cross entropy! (Although this may be because using sigmoid functions?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import *\n",
    "init_printing(use_unicode = True)\n",
    "CrossEntropy, y, x = symbols('CrossEntropy y x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CrossEntropy = -(y * log(x) + (1-y) * log(1-x))\n",
    "plot(CrossEntropy.subs({y:0.4}),(x,0.001,0.999));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimum of cross entropy is when x == y, as can be seen from the following:\n",
    "\n",
    "\\begin{align}\n",
    "d(CE) &= -(y x - (1-y)/(1-x))\\\\\n",
    "      &= -(y(1-x) - (1-y) x)/(x(1-x))\\\\\n",
    "      &= -(y - x)/x(1-x)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this is cool. But the cross entropy only works so far for the weights connecting to the output layer. I'm guessing that won't work for layers further back - they get another factor of the derivative of the activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incidentally, a note from Nielsen:\n",
    "\n",
    "*\"This, by the way, is part of a general pattern that we'll see through this chapter and, indeed, through much of the rest of the book. We'll develop a new technique, we'll try it out, and we'll get \"improved\" results. It is, of course, nice that we see such improvements. But the interpretation of such improvements is always problematic. They're only truly convincing if we see an improvement after putting tremendous effort into optimizing all the other hyper-parameters. That's a great deal of work, requiring lots of computing power, and we're not usually going to do such an exhaustive investigation. Instead, we'll proceed on the basis of informal tests like those done above. Still, you should keep in mind that such tests fall short of definitive proof, and remain alert to signs that the arguments are breaking down.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing \n",
    "\n",
    "Played with a 30 neuron network, training for aaaages. Get to 96.54% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax outputs\n",
    "\n",
    "Been reading further, gets on to softmax outputs. Useful as ensures that the outputs sum to 1. Also, seems to suppress other outputs, so hand wavily, perhaps makes more \"decisive\"\n",
    "\n",
    "Note that need to use log likelihood with softmax output layer to avoid learning slow down in the same way as for sigmoid above.\n",
    "\n",
    "Note from Nielsen:\n",
    "\n",
    "*\"Given this similarity, should you use a sigmoid output layer and cross-entropy, or a softmax output layer and log-likelihood? In fact, in many situations both approaches work well. Through the remainder of this chapter we'll use a sigmoid output layer, with the cross-entropy cost. Later, in Chapter 6, we'll sometimes use a softmax output layer, with log-likelihood cost. The reason for the switch is to make some of our later networks more similar to networks found in certain influential academic papers. As a more general point of principle, softmax plus log-likelihood is worth using whenever you want to interpret the output activations as probabilities. That's not always a concern, but can be useful with classification problems (like MNIST) involving disjoint classes.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "\n",
    "Overfitting is a big problem. Need to stop training when trainign data accuracy saturates.\n",
    "\n",
    "Validation data is used to fit hyper parameters. Split from test data for similar overfitting reasons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "*\"Increasing the amount of training data is one way of reducing overfitting. Are there other ways we can reduce the extent to which overfitting occurs? One possible approach is to reduce the size of our network. However, large networks have the potential to be more powerful than small networks, and so this is an option we'd only adopt reluctantly.\n",
    "Fortunately, there are other techniques which can reduce overfitting, even when we have a fixed network and fixed training data. These are known as regularization techniques.\"\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Regularization\n",
    "\n",
    "Add a cost term for the weights (quadratic). So weights decay \n",
    "\n",
    "Interesting note that not clear why should help necessarily (beyond vague feeling that smaller weights mean less sensitive to noise)\n",
    "\n",
    "*\"In fact, our networks already generalize better than one might a priori expect. A network with 100 hidden neurons has nearly 80,000 parameters. We have only 50,000 images in our training data. It's like trying to fit an 80,000th degree polynomial to 50,000 data points. By all rights, our network should overfit terribly. And yet, as we saw earlier, such a network actually does a pretty good job generalizing. Why is that the case? It's not well understood. It has been conjectured* *In Gradient-Based Learning Applied to Document Recognition, by Yann LeCun, LÃ©on Bottou, Yoshua Bengio, and Patrick Haffner (1998). that \"the dynamics of gradient descent learning in multilayer nets has a `self-regularization' effect\". This is exceptionally fortunate, but it's also somewhat disquieting that we don't understand why it's the case. In the meantime, we will adopt the pragmatic approach and use regularization whenever we can. Our neural networks will be the better for it.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1\n",
    "\n",
    "L1 similar, but shrinks weight towards zero by a constant amount (not proportional to weight). So doesn't pull down big weights as much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "Basically, also a form of regularization. Intuitive that avoids over fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "Basically, not much evidence which works best. Apparently rectified linear has been promising for image recognition. Alternative to sigmoid is tanh (similar, but allows for weights to a given neuron to increase and decrease at the same time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing gradient problem\n",
    "\n",
    "Interestingly, sigmoid does seem to be much worse than relu for deep neural networks in the sense that suffers from a vanishing gradient problem - deeper into the network, gradients tend to get smaller due to saturation effect of sigmoid. However, still likely that gradients rather unstable..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep neural networks\n",
    "\n",
    "Basically, not so much interesting. Conv is useful. We basically ignore the vanishing gradient problem. Etc etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
